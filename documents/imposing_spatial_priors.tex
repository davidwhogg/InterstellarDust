\documentclass[12pt, letterpaper]{article}

\newcommand{\given}{\,|\,}

\begin{document}

\noindent
to:  ES and HWR\\[1ex]
from:  DWH and DFM\\[1ex]
re: spatial priors\\[1ex]
~

Imagine you have $M$ two-dimensional pixels $m$ on the celestial sphere.
Within each pixel there are $N$ three three-dimensional boxels $mn$ in space along the line of sight
  (such that there are $[M\,N]$ total boxels in space).
Each boxel $mn$ has position $r_{mn}$ and we imagine it contains mean dust mass density $\rho_{mn}$.
We don't know these densities;
  they are parameters from our perspective.
For notational convenience we will define two other kinds of density objects:
One is a set of $M$ $N$-vectors $\rho_m$, one per sky pixel $m$,
  each of which is the vector made up of the $N$ values $\rho_{mn}$ in sky pixel $m$.
Another is the full $[M\,N]$-vector $\rho$,
  which is the single complete vector of all boxel densities $\rho_{mn}$.

Eddie and company have chosen some simple, vague prior on the $\rho_{mn}$
  (something like flat in $\rho_{mn}$ or $\ln\rho_{mn}$, and something like independent from boxel to boxel).
We will call this prior the ``interim prior'' in what follows.
For what follows, all we need to know are the following two things:
First, given the full $[M\,N]$-vector $\rho$,
  we can evaluate the interim prior $p_0(\rho)$.
Indeed, since Eddie and co.\ have sampled each sky pixel independently,
  this full-space prior is a product of individual-pixel priors.
Second, for each sky pixel $m$, we have a $K$-element sampling $\rho_{m}^{(k)}$,
  each of which is a $N$-vector $\rho_m$ drawn from the posterior pdf
  for the density along the line of sight in pixel $m$.

Now, one beautiful thing we will use is that,
  because each boxel $m$ has been independently sampled,
  and has independent data,
  and has an independent prior pdf over $\rho_m$,
  any combinatoric combination of the samplings is itself a sampling.
That is, the $K$ samplings per pixel $m$
  leads to a total of $K^M$ valid samplings for the full density $[M\,N]$-vector $\rho$.
Because the density is inferred from stars,
  because stars constrain integrals of the density,
  and because stars have noisy distance measurements,
  we only have noisy, correlated, non-trivial beliefs about the boxels $\rho_{mn}$ within sky pixel $m$,
  conditioned on this interim prior.

Imagine we want to---after the sampling under the interim prior has completed---%
  impose some more informative spatial prior pdf $p(\rho\given\alpha)$
  on the big density parameter vector $\rho$.
(Here $\alpha$ is a vector or blob of hyperparameters that control the informative spatial prior.)
Can we do this without going back to the original sampling?
Yes, we can, although it might hurt a bit.
It involves importance sampling.
The idea is as follows:
If we obtain a posterior sample $\rho^{(j)}$ of $\rho$ drawn from the posterior
  created with the interim prior $p_0(\rho)$,
  we can compute a weight $w_j$ for this sample
\begin{eqnarray}
w_j & \leftarrow & \frac{p(\rho^{(j)}\given\alpha)}{p_0(\rho^{(j)})}
\quad,
\end{eqnarray}
  where the weights are just the ratio of new prior to old prior (informative to interim).
The \emph{weighted} set of samples is, in principle,
  a sampling from the posterior pdf constructed under the informative prior.
Problem solved!

The problems are substantial:
The vast majority of samples we have of $\rho$ will have extremely low weights in this calculation.
There \emph{is} a vast number of samples of $\rho$ (combinatoric; see above).
It is very hard to find the small fraction (vanishing fraction) of samples that are ``important''
  under the importance sampling.
Another way to put it:
Imagine you \emph{rejection sample} using the importance weights
  (reject samples $j$ with an acceptance probability proportional to the weight $w_j$);
  the dynamic range will be so huge you will in general end up with only one sample at the end!

Of course another problem will be to specify $p(\rho\given\alpha)$.
But there we are going to use a Gaussian Process,
  to be discussed below,
  so for now let's presume that is easy (or at least possible).

The \textbf{dumbest possible option} is as follows:
For each pixel $m$, choose a $k_m$, which is one of the samples $\rho_m^{(k)}$.
The set of the $M$ $k_m$ samples $\rho_m^{(k)}$, taken together,
  effectively comprise a sample $\rho^{(j)}$ for the whole density field.
Compute the weight $w_j$ by taking the ratio of prior pdfs.
This is unlikely to work well for the combinatoric reasons mentioned above.

The \textbf{next dumbest thing} is a Gibbs-like variation on the above.
You start at a (randomly chosen) starting point $\rho^{(j)}$ which is itself
  chosen by choosing a set of $k_m$, one for every pixel $m$.
You then go to each pixel $m$ in turn, and you choose a new $k_m$ for that pixel,
  chosen with probability proportional to the relative posterior pdfs at the $k_m$ samples
  (and fixing or conditioning on the current sample or state of all the other pixels).
This would, in time, produce a rejection-sampled sampling of the new posterior pdf
  (under the informative prior).
We haven't proven that this would work, but I bet it would.
It would still be likely to be \emph{incredibly slow}.

The \textbf{next-to-next dumbest thing} would be to initialize that Gibbs run
  with a high-posterior-probability sampling chosen from among the combinatorially large
  number ($K^M$) of possible samples $\rho^{(j)}$.
One greedy algorithm would be to go to each pixel $m$ in turn,
  choosing the \emph{highest posterior pdf} sample $k_m$,
  given the state of all other pixels.
After a few passes through the data, this would converge
  to some locally optimal state.
It would be locally optimal because, by construction,
  every change made would increase the posterior pdf of the state.
This would also be super-fun, because it would embetterify the map in front of our eyes;
  indeed, the locally MAP dust model produced this way would be neat to look at.

All of these ideas are stil \textbf{dumb},
  because they involve repeated operations on $[M\,N]\times[M\,N]$ matrices,
  which are huge-normous.
That said, we can start with \emph{either}
  a small patch of the two-space (sky) or three-space (volume),
  chosen for testing and development;
  indeed it makes sense to choose an interesting patch.
Also, we can choose spatial priors (unlike Sale)
  that have compact spatial support;
  this leads to sparse matrices,
  which make computations (properly done) faster.

As for the spatial prior itself,
  our advice is to use a Gaussian Process,
  probably with a bell-shaped kernel (like a Gaussian shape or cosine or the like).
We can choose the kernel and its parameters via cross-validation.
We have very fast code for these kinds of problems.
That's not a guarantee that anything will really be possible,
  however it is a reason to start there in our development.

On the cross-validation point:
Another advantage of choosing a small test region of sky or space
  is that we can resample under different interim priors,
  or with left-out stars.
Our view is that you must have a sample of left-out stars
  (a validation sample)
  for setting or testing hyperparameter (and other) choices.
If ES has copious spare time,
  he might think about resampling a small patch of the sky
  (maybe 9 or 16 pixels in a square patch)
  leaving out one star per patch.
Even better to do that in an \emph{interesting} patch.

\textsl{Etc!}

\end{document}
